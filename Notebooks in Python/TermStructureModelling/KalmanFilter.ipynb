{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc24e93",
   "metadata": {},
   "source": [
    "<center> <h1> Implementation of Kalman Filter <h1/> <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2096b72",
   "metadata": {},
   "source": [
    "The implementation follows theory outlined in *Time Series Analysis* by Henrik Madsen (2007)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2a33d",
   "metadata": {},
   "source": [
    "### Theorems and Corollaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0edcac",
   "metadata": {},
   "source": [
    "**Theorem 2.6** (LINEAR PROJECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8765f8",
   "metadata": {},
   "source": [
    " $Y = (Y_{1},...Y_{m})^{T}$ and $X = (X_{1},...X_{n})^{T}$ be random vectors, and let the $(m + n)$-dimensional vector $(Y,X)^{T}$ have the mean and covariance\n",
    " \n",
    "$$\n",
    "(\\mu_{Y}, \\mu_{X})^{T} \\hspace{2cm} \\begin{bmatrix} \\Sigma_{YY} & \\Sigma_{YX} \\\\ \\Sigma_{XY} & \\Sigma_{XX} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Define the linear projection of $Y$ on $X$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X|Y] = a + BX\n",
    "$$\n",
    "\n",
    "Then the projection - and the variance of the projection error - is given by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[Y | X] &= \\mu_{Y} + \\Sigma_{YX} \\Sigma_{XX}^{-1}(X - \\mu_{X}) \\\\\n",
    "    \\mathbb{E}[\\mathbb{V}[Y | X]] &= \\Sigma_{YY} - \\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{YX}^{T} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, the projection error, $Y - \\mathbb{E}[Y | X]$ and $X$ are uncorrelated. Hence, $\\mathbb{COV}[Y - \\mathbb{E}[Y | X], X] = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca2166",
   "metadata": {},
   "source": [
    "**Theorem 2.8** (PROJECTIONS FOR NORMALLY DISTRIBUTED VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfeba89",
   "metadata": {},
   "source": [
    "Let $(Y^{T}, X^{T})^{T}$ be normally distributed random variable with mean $(\\mu_{x\n",
    "Y}, \\mu_{X})^{T}$ and covariance $\\begin{bmatrix} \\Sigma_{YY} & \\Sigma_{YX} \\\\ \\Sigma_{XY} & \\Sigma_{XX} \\end{bmatrix}$. Then the r.v. $Y | X$ is normally distributed with mean and variance given by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[Y | X] &= \\mu_{Y} + \\Sigma_{YX} \\Sigma_{XX}^{-1}(X - \\mu_{X}) \\\\\n",
    "    \\mathbb{V}[Y | X] &= \\Sigma_{YY} - \\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{YX}^{T} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Furthermore, $(Y - \\mathbb{E}[Y|X])$ and $X$ are independent. That means $\\mathbb{COV}[(Y - \\mathbb{E}[Y|X]), X] = 0$. Proof can be found is Jazwinski (1970).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f923f9",
   "metadata": {},
   "source": [
    "**Theorem 3.9** (OPTIMAL PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071abf4c",
   "metadata": {},
   "source": [
    "It holds that \n",
    "\n",
    "$$\n",
    "\\min_{g} \\mathbb{E}[(Y - g(X))^{2} | X = x] = \\mathbb{E}[(Y - g^{*}(X))^{2} | X = x]\n",
    "$$\n",
    "\n",
    "where $g^{*}(x) = \\mathbb{E}[Y | X = x]$. Hence, if we want to minimize the squared prediction error, then *the optimal prediction is equal to the conditional expectation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84937b",
   "metadata": {},
   "source": [
    "### State Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029bd285",
   "metadata": {},
   "source": [
    "Consider the linear stochastic state space model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    X_{t} &= A X_{t-1} + B u_{t-1} + e_{1,t}  \\hspace{1.1cm} (1) \\\\\n",
    "    Y_{t} &= C X_{t} + e_{2,t} \\hspace{3cm} (2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f35e3c",
   "metadata": {},
   "source": [
    "The state variables $X_{t}, Y_{t} \\in \\mathbb{R}^{n}$, and the exogenous variables $u_{t-1} \\in \\mathbb{R}^{m}$. Therefore, $A \\in \\mathbb{R}^{n \\times n}, B \\in \\mathbb{R}^{n \\times m}, C \\in \\mathbb{n \\times n}, e_{1,t} \\in \\mathbb{n \\times 1}$ and $e_{2,t} \\in \\mathbb{R}^{n \\times 1}$. The interpretation is that we have an $\\textit{unobserved}$ linear stochastic process $X_{t}$ with deterministic inputs $u_{t-1}$ that is affected by white noise $e_{1,t}$. We observe $X_{t}$ discretely through $Y_{t}$, where $e_{2,t}$ is the $\\textit{measurement error}$. Equation (1) is called the *system equation* and Equation (2) is called the *observation equation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b408701",
   "metadata": {},
   "source": [
    "Let $\\mathcal{Y}_{t}^{T} = (Y_{1}^{T},...Y_{t}^{T})$. Assume that $X_{t}|\\mathcal{Y}_{t}$ is normally distributed. Then the distribution is completely characterized by $\\hat X_{t|t} = \\mathbb{E}[X_{t}|\\mathcal{Y}_{t}]$ and $\\hat \\Sigma^{xx}_{t|t} = \\mathbb{V}[X_{t}|\\mathcal{Y}_{t}]$, where $\\mathcal{Y}_{t}$ denote the non-missing observations up to time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460da92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
